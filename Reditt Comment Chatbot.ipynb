{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc34e7fb",
   "metadata": {},
   "source": [
    "### This purpose of this project is to build a chatbot that can uncover insights about Reddit comment about \"Data Science\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db932f0f",
   "metadata": {},
   "source": [
    "#### Section 1: Get Reditt posts related to data science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd780ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import reddit api and other libaries\n",
    "import praw\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce53b60c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>post_url</th>\n",
       "      <th>post_title</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gh1dj9</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.589117e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/v492uoheuxx41</td>\n",
       "      <td>[Project] From books to presentations in 10s w...</td>\n",
       "      <td>Project</td>\n",
       "      <td>7847</td>\n",
       "      <td>186</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kuc6tz</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.610275e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/25nxi9ojfha61</td>\n",
       "      <td>[D] A Demo from 1993 of 32-year-old Yann LeCun...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>5878</td>\n",
       "      <td>133</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g7nfvb</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.587789e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/rlmmjm1q5wu41</td>\n",
       "      <td>[R] First Order Motion Model applied to animat...</td>\n",
       "      <td>Research</td>\n",
       "      <td>4760</td>\n",
       "      <td>111</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lui92h</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.614525e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/ikd5gjlbi8k61</td>\n",
       "      <td>[N] AI can turn old photos into moving Images ...</td>\n",
       "      <td>News</td>\n",
       "      <td>4702</td>\n",
       "      <td>230</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ohxnts</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.625977e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://i.redd.it/34sgziebfia71.jpg</td>\n",
       "      <td>[D] This AI reveals how much time politicians ...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>4602</td>\n",
       "      <td>228</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>cqffii</td>\n",
       "      <td>datascience</td>\n",
       "      <td>1.565815e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://i.redd.it/4f71u8ti5hg31.jpg</td>\n",
       "      <td>Expectation vs reality</td>\n",
       "      <td>Fun/Trivia</td>\n",
       "      <td>1780</td>\n",
       "      <td>94</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>g6og9l</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.587655e+09</td>\n",
       "      <td># DICK-RNN\\n\\nA recurrent neural network train...</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>[P] I trained a recurrent neural network train...</td>\n",
       "      <td>Project</td>\n",
       "      <td>1775</td>\n",
       "      <td>123</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ijkkbb</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.598822e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/47g1f9cuf7k51</td>\n",
       "      <td>[P] Cross-Model Interpolations between 5 Style...</td>\n",
       "      <td>Project</td>\n",
       "      <td>1773</td>\n",
       "      <td>104</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>orybjg</td>\n",
       "      <td>datascience</td>\n",
       "      <td>1.627305e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://i.redd.it/u3ngf9tw2kd71.png</td>\n",
       "      <td>Me showing off a suspiciously well-performing ...</td>\n",
       "      <td>Fun/Trivia</td>\n",
       "      <td>1760</td>\n",
       "      <td>27</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>xtd8kc</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>1.664678e+09</td>\n",
       "      <td></td>\n",
       "      <td>https://v.redd.it/w00lkjcl0br91</td>\n",
       "      <td>[P] stablediffusion-infinity: Outpainting with...</td>\n",
       "      <td>Project</td>\n",
       "      <td>1739</td>\n",
       "      <td>57</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id        subreddit   created_utc  \\\n",
       "0   gh1dj9  MachineLearning  1.589117e+09   \n",
       "1   kuc6tz  MachineLearning  1.610275e+09   \n",
       "2   g7nfvb  MachineLearning  1.587789e+09   \n",
       "3   lui92h  MachineLearning  1.614525e+09   \n",
       "4   ohxnts  MachineLearning  1.625977e+09   \n",
       "..     ...              ...           ...   \n",
       "95  cqffii      datascience  1.565815e+09   \n",
       "96  g6og9l  MachineLearning  1.587655e+09   \n",
       "97  ijkkbb  MachineLearning  1.598822e+09   \n",
       "98  orybjg      datascience  1.627305e+09   \n",
       "99  xtd8kc  MachineLearning  1.664678e+09   \n",
       "\n",
       "                                             selftext  \\\n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "3                                                       \n",
       "4                                                       \n",
       "..                                                ...   \n",
       "95                                                      \n",
       "96  # DICK-RNN\\n\\nA recurrent neural network train...   \n",
       "97                                                      \n",
       "98                                                      \n",
       "99                                                      \n",
       "\n",
       "                                             post_url  \\\n",
       "0                     https://v.redd.it/v492uoheuxx41   \n",
       "1                     https://v.redd.it/25nxi9ojfha61   \n",
       "2                     https://v.redd.it/rlmmjm1q5wu41   \n",
       "3                     https://v.redd.it/ikd5gjlbi8k61   \n",
       "4                 https://i.redd.it/34sgziebfia71.jpg   \n",
       "..                                                ...   \n",
       "95                https://i.redd.it/4f71u8ti5hg31.jpg   \n",
       "96  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "97                    https://v.redd.it/47g1f9cuf7k51   \n",
       "98                https://i.redd.it/u3ngf9tw2kd71.png   \n",
       "99                    https://v.redd.it/w00lkjcl0br91   \n",
       "\n",
       "                                           post_title link_flair_text  score  \\\n",
       "0   [Project] From books to presentations in 10s w...         Project   7847   \n",
       "1   [D] A Demo from 1993 of 32-year-old Yann LeCun...      Discussion   5878   \n",
       "2   [R] First Order Motion Model applied to animat...        Research   4760   \n",
       "3   [N] AI can turn old photos into moving Images ...            News   4702   \n",
       "4   [D] This AI reveals how much time politicians ...      Discussion   4602   \n",
       "..                                                ...             ...    ...   \n",
       "95                             Expectation vs reality      Fun/Trivia   1780   \n",
       "96  [P] I trained a recurrent neural network train...         Project   1775   \n",
       "97  [P] Cross-Model Interpolations between 5 Style...         Project   1773   \n",
       "98  Me showing off a suspiciously well-performing ...      Fun/Trivia   1760   \n",
       "99  [P] stablediffusion-infinity: Outpainting with...         Project   1739   \n",
       "\n",
       "    num_comments  upvote_ratio  \n",
       "0            186          0.99  \n",
       "1            133          0.98  \n",
       "2            111          0.97  \n",
       "3            230          0.97  \n",
       "4            228          0.96  \n",
       "..           ...           ...  \n",
       "95            94          0.96  \n",
       "96           123          0.96  \n",
       "97           104          0.97  \n",
       "98            27          0.98  \n",
       "99            57          0.98  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_posts(subreddit_list, limit=500, time_filter='all'):\n",
    "\n",
    "    # call reddit api using my api client_id, client_secret, and user_agent, which have been set as environment variables\n",
    "    reddit = praw.Reddit(client_id = os.environ['reddit_client'],\n",
    "                     client_secret = os.environ['reddit_secret'],\n",
    "                     user_agent = os.environ['reddit_user'],\n",
    "                     redirect_url = 'http://localhost:8080')\n",
    "    \n",
    "    # extract posts that are in given subreddit from api\n",
    "    posts = reddit.subreddit(subreddit_list).top(time_filter=time_filter, limit=limit)\n",
    "\n",
    "    # initialize post dataframe\n",
    "    posts_df = []\n",
    "\n",
    "    # add post attributes to dataframe for retrieving relevant information from other api call later\n",
    "    for post in posts:\n",
    "        posts_df.append({'post_id': post.id,\n",
    "                        'subreddit': post.subreddit,\n",
    "                        'created_utc': post.created_utc,\n",
    "                        'selftext': post.selftext,\n",
    "                        'post_url': post.url,\n",
    "                        'post_title': post.title,\n",
    "                        'link_flair_text': post.link_flair_text,\n",
    "                        'score': post.score,\n",
    "                        'num_comments': post.num_comments,\n",
    "                        'upvote_ratio': post.upvote_ratio\n",
    "                        })\n",
    "        \n",
    "    return pd.DataFrame(posts_df)\n",
    "\n",
    "\n",
    "# retrieve Top 100 posts from subreddit \"MachineLearning\", \"artificial\", \"data\"\n",
    "posts_df = get_top_posts(subreddit_list='MachineLearning+artificial+datascience', limit=100, time_filter='all')\n",
    "posts_df.to_csv('DS_ML_AI_posts.csv', header=True, index=False)\n",
    "posts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f50169",
   "metadata": {},
   "source": [
    "#### Section 2: Get comments of these posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbc723c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gh1dj9</td>\n",
       "      <td>Twitter thread: [https://twitter.com/cyrildiag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gh1dj9</td>\n",
       "      <td>The future ðŸ¤¯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gh1dj9</td>\n",
       "      <td>Simple yet very useful. Thank you for sharing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gh1dj9</td>\n",
       "      <td>Almost guaranteed, Apple will copy your idea i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gh1dj9</td>\n",
       "      <td>Ohh the nightmare of making this into a stable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13612</th>\n",
       "      <td>xtd8kc</td>\n",
       "      <td>I don't think we are compatible with the webui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13613</th>\n",
       "      <td>xtd8kc</td>\n",
       "      <td>Cool. Iâ€™ve already accepted it but I can doubl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13614</th>\n",
       "      <td>xtd8kc</td>\n",
       "      <td>You need the token even for the local one. (It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13615</th>\n",
       "      <td>xtd8kc</td>\n",
       "      <td>No worries, Iâ€™ve got a token. ðŸ˜‚\\nJust installe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13616</th>\n",
       "      <td>xtd8kc</td>\n",
       "      <td>If you copy the error message here I might be ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13617 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id                                            comment\n",
       "0      gh1dj9  Twitter thread: [https://twitter.com/cyrildiag...\n",
       "1      gh1dj9                                       The future ðŸ¤¯\n",
       "2      gh1dj9  Simple yet very useful. Thank you for sharing ...\n",
       "3      gh1dj9  Almost guaranteed, Apple will copy your idea i...\n",
       "4      gh1dj9  Ohh the nightmare of making this into a stable...\n",
       "...       ...                                                ...\n",
       "13612  xtd8kc  I don't think we are compatible with the webui...\n",
       "13613  xtd8kc  Cool. Iâ€™ve already accepted it but I can doubl...\n",
       "13614  xtd8kc  You need the token even for the local one. (It...\n",
       "13615  xtd8kc  No worries, Iâ€™ve got a token. ðŸ˜‚\\nJust installe...\n",
       "13616  xtd8kc  If you copy the error message here I might be ...\n",
       "\n",
       "[13617 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list to store comments of all posts\n",
    "comments_list = []\n",
    "for post_id in posts_df['post_id']:\n",
    "    \n",
    "    # call comment api\n",
    "    submission = reddit.submission(post_id)\n",
    "    \n",
    "    # in case there are \"more comments\" section, retrieve 10 more\n",
    "    submission.comments.replace_more(limit = 10)\n",
    "    \n",
    "    # map comment with post\n",
    "    for comment in submission.comments.list():\n",
    "        comments_list.append({'post_id':post_id, 'comment':comment.body})\n",
    "\n",
    "comments_df = pd.DataFrame(comments_list)\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866f289",
   "metadata": {},
   "source": [
    "#### Section 3: Build chatbot to answer questions based on the retrieved comment using OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdecb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import llama_index libaries to convert comment text into index that can be used by AI\n",
    "from llama_index import SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor,PromptHelper\n",
    "\n",
    "# import OpenAI to call AI engine\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd7fa9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all comments and save to a local file\n",
    "comment_text = ' '.join(comments_df['comment'])\n",
    "f = open(\"textdata/all_text_reddit.txt\", \"w\") \n",
    "f.write(comment_text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9acec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert comment text into index to be used by AI\n",
    "def construct_index(directory_path):\n",
    "    \n",
    "    # set token limits for input and output to avoid long queries and long answers (it costs money!)\n",
    "    max_input_size = 4096\n",
    "    num_outputs = 256\n",
    "\n",
    "    # call GPT 3.5 turbo model to answer questions\n",
    "    llm_predictor = LLMPredictor(llm = OpenAI(temperature = 0, \n",
    "                                              model_name = 'gpt-3.5-turbo',\n",
    "                                             max_token = num_outputs))\n",
    "    \n",
    "    # restrict prompts delievered to the model under the hood to save money and increae speed\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs)\n",
    "    \n",
    "    # store argument defiuned above\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "    \n",
    "    # turn local file into document objects for creating index\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    \n",
    "    # build index and save to local file\n",
    "    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n",
    "    index.save_to_disk('index.json')\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "632ac2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \"ask_me_anything\" function as user interface\n",
    "def ask_me_anything(question):\n",
    "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "    response = index.query(question, response_mode = 'compact')\n",
    "    print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9a4100c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 664312 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.vector_indices.GPTSimpleVectorIndex at 0x7fa9d1199bb0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load comment data to build index\n",
    "construct_index('textdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe5275c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3945 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The trend of machine learning is that it is steadily improving over time as long as the input data set is consistent and the evaluation of the performance is correct. This is achieved by using programs that guess and check at scale, with the results being graded and aggregated into better guesses. This process is repeated until the desired outcome is achieved. Machine learning is also being used to identify and mitigate cognitive biases such as regression towards the mean and the gambler's fallacy, which can help improve the accuracy of predictions and decisions.\n"
     ]
    }
   ],
   "source": [
    "ask_me_anything(\"What's the trend of machine leanring\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
